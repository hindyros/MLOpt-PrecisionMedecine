{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f1a32d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames, CategoricalArrays, Plots, Statistics, Random, StatsPlots, Gurobi, JuMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "66e93453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 4)"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>20×4 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Age</th><th style = \"text-align: left;\">DMRxAge</th><th style = \"text-align: left;\">BMI_mean</th><th style = \"text-align: left;\">HbA1c</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">67.7974</td><td style = \"text-align: right;\">9.30869</td><td style = \"text-align: right;\">40.4315</td><td style = \"text-align: right;\">7.2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">71.2909</td><td style = \"text-align: right;\">3.95346</td><td style = \"text-align: right;\">38.0979</td><td style = \"text-align: right;\">8.4</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">51.3101</td><td style = \"text-align: right;\">5.36619</td><td style = \"text-align: right;\">30.3333</td><td style = \"text-align: right;\">11.6</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">64.4901</td><td style = \"text-align: right;\">0.821355</td><td style = \"text-align: right;\">48.5597</td><td style = \"text-align: right;\">8.1</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">69.859</td><td style = \"text-align: right;\">8.95003</td><td style = \"text-align: right;\">42.3325</td><td style = \"text-align: right;\">12.6</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">82.82</td><td style = \"text-align: right;\">1.05133</td><td style = \"text-align: right;\">32.305</td><td style = \"text-align: right;\">9.2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">51.5072</td><td style = \"text-align: right;\">7.78919</td><td style = \"text-align: right;\">26.8044</td><td style = \"text-align: right;\">7.7</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">37.9822</td><td style = \"text-align: right;\">2.73785</td><td style = \"text-align: right;\">31.0588</td><td style = \"text-align: right;\">10.7</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">38.4203</td><td style = \"text-align: right;\">1.67283</td><td style = \"text-align: right;\">27.7456</td><td style = \"text-align: right;\">9.6</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">53.18</td><td style = \"text-align: right;\">3.24162</td><td style = \"text-align: right;\">31.1516</td><td style = \"text-align: right;\">7.5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">60.345</td><td style = \"text-align: right;\">6.71595</td><td style = \"text-align: right;\">32.2257</td><td style = \"text-align: right;\">8.9</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">53.2183</td><td style = \"text-align: right;\">5.82615</td><td style = \"text-align: right;\">33.701</td><td style = \"text-align: right;\">9.2</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">46.3847</td><td style = \"text-align: right;\">5.86721</td><td style = \"text-align: right;\">32.9263</td><td style = \"text-align: right;\">9.5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">14</td><td style = \"text-align: right;\">48.0657</td><td style = \"text-align: right;\">9.30869</td><td style = \"text-align: right;\">23.8571</td><td style = \"text-align: right;\">11.9</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">15</td><td style = \"text-align: right;\">82.3025</td><td style = \"text-align: right;\">7.66598</td><td style = \"text-align: right;\">28.7521</td><td style = \"text-align: right;\">9.7</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">16</td><td style = \"text-align: right;\">69.7577</td><td style = \"text-align: right;\">1.36619</td><td style = \"text-align: right;\">33.6318</td><td style = \"text-align: right;\">9.7</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">17</td><td style = \"text-align: right;\">63.2635</td><td style = \"text-align: right;\">4.89802</td><td style = \"text-align: right;\">37.9153</td><td style = \"text-align: right;\">6.5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">18</td><td style = \"text-align: right;\">43.2526</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">25.4362</td><td style = \"text-align: right;\">10.5</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">19</td><td style = \"text-align: right;\">58.6913</td><td style = \"text-align: right;\">0.550308</td><td style = \"text-align: right;\">38.7224</td><td style = \"text-align: right;\">7.8</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">20</td><td style = \"text-align: right;\">67.5318</td><td style = \"text-align: right;\">1.64271</td><td style = \"text-align: right;\">35.5555</td><td style = \"text-align: right;\">7.8</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& Age & DMRxAge & BMI\\_mean & HbA1c\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 67.7974 & 9.30869 & 40.4315 & 7.2 \\\\\n",
       "\t2 & 71.2909 & 3.95346 & 38.0979 & 8.4 \\\\\n",
       "\t3 & 51.3101 & 5.36619 & 30.3333 & 11.6 \\\\\n",
       "\t4 & 64.4901 & 0.821355 & 48.5597 & 8.1 \\\\\n",
       "\t5 & 69.859 & 8.95003 & 42.3325 & 12.6 \\\\\n",
       "\t6 & 82.82 & 1.05133 & 32.305 & 9.2 \\\\\n",
       "\t7 & 51.5072 & 7.78919 & 26.8044 & 7.7 \\\\\n",
       "\t8 & 37.9822 & 2.73785 & 31.0588 & 10.7 \\\\\n",
       "\t9 & 38.4203 & 1.67283 & 27.7456 & 9.6 \\\\\n",
       "\t10 & 53.18 & 3.24162 & 31.1516 & 7.5 \\\\\n",
       "\t11 & 60.345 & 6.71595 & 32.2257 & 8.9 \\\\\n",
       "\t12 & 53.2183 & 5.82615 & 33.701 & 9.2 \\\\\n",
       "\t13 & 46.3847 & 5.86721 & 32.9263 & 9.5 \\\\\n",
       "\t14 & 48.0657 & 9.30869 & 23.8571 & 11.9 \\\\\n",
       "\t15 & 82.3025 & 7.66598 & 28.7521 & 9.7 \\\\\n",
       "\t16 & 69.7577 & 1.36619 & 33.6318 & 9.7 \\\\\n",
       "\t17 & 63.2635 & 4.89802 & 37.9153 & 6.5 \\\\\n",
       "\t18 & 43.2526 & 0.0 & 25.4362 & 10.5 \\\\\n",
       "\t19 & 58.6913 & 0.550308 & 38.7224 & 7.8 \\\\\n",
       "\t20 & 67.5318 & 1.64271 & 35.5555 & 7.8 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m20×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Age     \u001b[0m\u001b[1m DMRxAge  \u001b[0m\u001b[1m BMI_mean \u001b[0m\u001b[1m HbA1c   \u001b[0m\n",
       "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼──────────────────────────────────────\n",
       "   1 │ 67.7974  9.30869    40.4315      7.2\n",
       "   2 │ 71.2909  3.95346    38.0979      8.4\n",
       "   3 │ 51.3101  5.36619    30.3333     11.6\n",
       "   4 │ 64.4901  0.821355   48.5597      8.1\n",
       "   5 │ 69.859   8.95003    42.3325     12.6\n",
       "   6 │ 82.82    1.05133    32.305       9.2\n",
       "   7 │ 51.5072  7.78919    26.8044      7.7\n",
       "   8 │ 37.9822  2.73785    31.0588     10.7\n",
       "   9 │ 38.4203  1.67283    27.7456      9.6\n",
       "  10 │ 53.18    3.24162    31.1516      7.5\n",
       "  11 │ 60.345   6.71595    32.2257      8.9\n",
       "  12 │ 53.2183  5.82615    33.701       9.2\n",
       "  13 │ 46.3847  5.86721    32.9263      9.5\n",
       "  14 │ 48.0657  9.30869    23.8571     11.9\n",
       "  15 │ 82.3025  7.66598    28.7521      9.7\n",
       "  16 │ 69.7577  1.36619    33.6318      9.7\n",
       "  17 │ 63.2635  4.89802    37.9153      6.5\n",
       "  18 │ 43.2526  0.0        25.4362     10.5\n",
       "  19 │ 58.6913  0.550308   38.7224      7.8\n",
       "  20 │ 67.5318  1.64271    35.5555      7.8"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"data_diabetes.csv\", DataFrame)\n",
    "print(size(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf264c2",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "**Objective:** Split 20 numbers into two groups of 10 each to minimize discrepancies in centered first and second moments.\n",
    "\n",
    "**Methods to implement:**\n",
    "1. **Randomization:** Shuffle all numbers, split first half to group 1, rest to group 2\n",
    "2. **Re-randomization:** Do randomization 10,000 times, choose the one with lowest sum of absolute differences of first and second moments\n",
    "3. **Pair Matching:** Rank all numbers, for every consecutive pair, randomly split them into groups\n",
    "4. **Optimization:** Use MIO formulation from lecture notes with ρ = 0.5\n",
    "\n",
    "**Metrics to report:**\n",
    "- Total discrepancy: |μ_p(x) - μ_q(x)| + |σ²_p(x) - σ²_q(x)|\n",
    "- Mean difference: |μ_p(x) - μ_q(x)|\n",
    "- Variance difference: |σ²_p(x) - σ²_q(x)|\n",
    "\n",
    "**Random seed:** 15095\n",
    "\n",
    "---\n",
    "\n",
    "#### Results and Discussion\n",
    "\n",
    "The results demonstrate a clear hierarchy in the effectiveness of different allocation methods for minimizing discrepancy in BMI between the two groups:\n",
    "\n",
    "1. **Randomization** achieved a total discrepancy of **1.582**, with mean difference of 0.890 and variance difference of 0.692. This represents the baseline performance of a single random allocation, which shows substantial imbalance between groups.\n",
    "\n",
    "2. **Re-randomization** (selecting the best of 10,000 randomizations) dramatically improved the discrepancy to **0.0049**, achieving near-perfect balance with mean difference of 0.0003 and variance difference of 0.0046. This demonstrates that while randomization can achieve good balance, it requires many trials to find a favorable allocation.\n",
    "\n",
    "3. **Pair Matching** achieved a total discrepancy of **0.467**, which is substantially better than single randomization but worse than re-randomization. The method balances mean difference (0.156) and variance difference (0.311) reasonably well by pairing similar observations and randomly assigning within pairs, providing a computationally efficient heuristic.\n",
    "\n",
    "4. **Optimization (MIO)** achieved the lowest total discrepancy of **0.0049**, matching the performance of re-randomization. The optimization approach systematically finds the optimal allocation that minimizes the weighted sum of mean and variance differences (with ρ = 0.5), demonstrating that mathematical optimization can achieve the same quality as extensive random search but with guaranteed optimality.\n",
    "\n",
    "**Key Insights:**\n",
    "- The optimization approach provides **guaranteed optimality** without requiring thousands of random trials, making it computationally efficient and reliable.\n",
    "- Re-randomization achieves similar performance to optimization but requires 10,000 evaluations, whereas optimization solves the problem directly.\n",
    "- Pair matching offers a good compromise: better than single randomization and computationally simpler than optimization, making it useful when optimization is not feasible.\n",
    "- The dramatic improvement from randomization (1.582) to optimization (0.0049) highlights the value of systematic approaches over naive random assignment in experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94b40af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_discrepancy (generic function with 1 method)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variance function since julia does it over n-1 and not n\n",
    "# Population variance: σ² = Σ(x_i - μ)² / n\n",
    "function var_n(x)\n",
    "    m = mean(x)\n",
    "    return sum((x .- m).^2) / length(x)\n",
    "end\n",
    "\n",
    "# Function to calculate discrepancy between two groups\n",
    "# Discrepancy = |μ₁ - μ₂| + |σ²₁ - σ²₂|\n",
    "# where μ is the mean (first moment) and σ² is the variance (second moment)\n",
    "function calculate_discrepancy(group1, group2)\n",
    "    mu1 = mean(group1)\n",
    "    mu2 = mean(group2)\n",
    "    var1 = var_n(group1)  # Population variance (divide by n)\n",
    "    var2 = var_n(group2)  # Population variance (divide by n)\n",
    "    \n",
    "    diff_mean = abs(mu1 - mu2)\n",
    "    diff_var = abs(var1 - var2)\n",
    "    total_discrepancy = diff_mean + diff_var\n",
    "    \n",
    "    return total_discrepancy, diff_mean, diff_var\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cd963e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Vector{Float64}:\n",
       "  1.1517823114246133\n",
       "  0.759646036796874\n",
       " -0.5450886769285921\n",
       "  2.5176181959375086\n",
       "  1.471213977317227\n",
       " -0.21377594085764492\n",
       " -1.1380721924618067\n",
       " -0.42317727274534983\n",
       " -0.9799149537651085\n",
       " -0.4075939801910288\n",
       " -0.22709886501674117\n",
       "  0.020803558097440788\n",
       " -0.1093830227159202\n",
       " -1.6333275849445663\n",
       " -0.810791965357081\n",
       "  0.009180313641475379\n",
       "  0.7289539744236824\n",
       " -1.367988311470946\n",
       "  0.8645920366006654\n",
       "  0.33242236221529914"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom normalizer function following Lecture 14\n",
    "# w'_i = (y_i - μ̂) / σ̂ where μ̂ = Σy_i/n and σ̂² = Σ(y_i - μ̂)²/n\n",
    "# Use population variance (divide by n, not n-1)\n",
    "function custom_normalizer(y)\n",
    "    mu_hat = mean(y)  # μ̂ = Σy_i/n\n",
    "    sigma_squared_hat = var_n(y)  # σ̂² = Σ(y_i - μ̂)²/n (population variance)\n",
    "    sigma_hat = sqrt(sigma_squared_hat)  # σ̂ = sqrt(σ̂²)\n",
    "    w_prime = (y .- mu_hat) ./ sigma_hat  # w'_i = (y_i - μ̂) / σ̂\n",
    "    return w_prime\n",
    "end\n",
    "\n",
    "# Preprocessing: Apply custom normalizer to both HbA1c and BMI_mean\n",
    "data.HbA1c = custom_normalizer(data.HbA1c)\n",
    "data.BMI_mean = custom_normalizer(data.BMI_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb21cbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>20×4 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Age</th><th style = \"text-align: left;\">DMRxAge</th><th style = \"text-align: left;\">BMI_mean</th><th style = \"text-align: left;\">HbA1c</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">67.7974</td><td style = \"text-align: right;\">9.30869</td><td style = \"text-align: right;\">1.15178</td><td style = \"text-align: right;\">-1.24839</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">71.2909</td><td style = \"text-align: right;\">3.95346</td><td style = \"text-align: right;\">0.759646</td><td style = \"text-align: right;\">-0.501222</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">51.3101</td><td style = \"text-align: right;\">5.36619</td><td style = \"text-align: right;\">-0.545089</td><td style = \"text-align: right;\">1.49121</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">64.4901</td><td style = \"text-align: right;\">0.821355</td><td style = \"text-align: right;\">2.51762</td><td style = \"text-align: right;\">-0.688013</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">69.859</td><td style = \"text-align: right;\">8.95003</td><td style = \"text-align: right;\">1.47121</td><td style = \"text-align: right;\">2.11385</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">82.82</td><td style = \"text-align: right;\">1.05133</td><td style = \"text-align: right;\">-0.213776</td><td style = \"text-align: right;\">-0.00311318</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">51.5072</td><td style = \"text-align: right;\">7.78919</td><td style = \"text-align: right;\">-1.13807</td><td style = \"text-align: right;\">-0.937067</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">37.9822</td><td style = \"text-align: right;\">2.73785</td><td style = \"text-align: right;\">-0.423177</td><td style = \"text-align: right;\">0.930841</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">38.4203</td><td style = \"text-align: right;\">1.67283</td><td style = \"text-align: right;\">-0.979915</td><td style = \"text-align: right;\">0.245941</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">53.18</td><td style = \"text-align: right;\">3.24162</td><td style = \"text-align: right;\">-0.407594</td><td style = \"text-align: right;\">-1.06159</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">60.345</td><td style = \"text-align: right;\">6.71595</td><td style = \"text-align: right;\">-0.227099</td><td style = \"text-align: right;\">-0.189904</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">53.2183</td><td style = \"text-align: right;\">5.82615</td><td style = \"text-align: right;\">0.0208036</td><td style = \"text-align: right;\">-0.00311318</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">46.3847</td><td style = \"text-align: right;\">5.86721</td><td style = \"text-align: right;\">-0.109383</td><td style = \"text-align: right;\">0.183678</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">14</td><td style = \"text-align: right;\">48.0657</td><td style = \"text-align: right;\">9.30869</td><td style = \"text-align: right;\">-1.63333</td><td style = \"text-align: right;\">1.678</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">15</td><td style = \"text-align: right;\">82.3025</td><td style = \"text-align: right;\">7.66598</td><td style = \"text-align: right;\">-0.810792</td><td style = \"text-align: right;\">0.308205</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">16</td><td style = \"text-align: right;\">69.7577</td><td style = \"text-align: right;\">1.36619</td><td style = \"text-align: right;\">0.00918031</td><td style = \"text-align: right;\">0.308205</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">17</td><td style = \"text-align: right;\">63.2635</td><td style = \"text-align: right;\">4.89802</td><td style = \"text-align: right;\">0.728954</td><td style = \"text-align: right;\">-1.68423</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">18</td><td style = \"text-align: right;\">43.2526</td><td style = \"text-align: right;\">0.0</td><td style = \"text-align: right;\">-1.36799</td><td style = \"text-align: right;\">0.806314</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">19</td><td style = \"text-align: right;\">58.6913</td><td style = \"text-align: right;\">0.550308</td><td style = \"text-align: right;\">0.864592</td><td style = \"text-align: right;\">-0.874804</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">20</td><td style = \"text-align: right;\">67.5318</td><td style = \"text-align: right;\">1.64271</td><td style = \"text-align: right;\">0.332422</td><td style = \"text-align: right;\">-0.874804</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& Age & DMRxAge & BMI\\_mean & HbA1c\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 67.7974 & 9.30869 & 1.15178 & -1.24839 \\\\\n",
       "\t2 & 71.2909 & 3.95346 & 0.759646 & -0.501222 \\\\\n",
       "\t3 & 51.3101 & 5.36619 & -0.545089 & 1.49121 \\\\\n",
       "\t4 & 64.4901 & 0.821355 & 2.51762 & -0.688013 \\\\\n",
       "\t5 & 69.859 & 8.95003 & 1.47121 & 2.11385 \\\\\n",
       "\t6 & 82.82 & 1.05133 & -0.213776 & -0.00311318 \\\\\n",
       "\t7 & 51.5072 & 7.78919 & -1.13807 & -0.937067 \\\\\n",
       "\t8 & 37.9822 & 2.73785 & -0.423177 & 0.930841 \\\\\n",
       "\t9 & 38.4203 & 1.67283 & -0.979915 & 0.245941 \\\\\n",
       "\t10 & 53.18 & 3.24162 & -0.407594 & -1.06159 \\\\\n",
       "\t11 & 60.345 & 6.71595 & -0.227099 & -0.189904 \\\\\n",
       "\t12 & 53.2183 & 5.82615 & 0.0208036 & -0.00311318 \\\\\n",
       "\t13 & 46.3847 & 5.86721 & -0.109383 & 0.183678 \\\\\n",
       "\t14 & 48.0657 & 9.30869 & -1.63333 & 1.678 \\\\\n",
       "\t15 & 82.3025 & 7.66598 & -0.810792 & 0.308205 \\\\\n",
       "\t16 & 69.7577 & 1.36619 & 0.00918031 & 0.308205 \\\\\n",
       "\t17 & 63.2635 & 4.89802 & 0.728954 & -1.68423 \\\\\n",
       "\t18 & 43.2526 & 0.0 & -1.36799 & 0.806314 \\\\\n",
       "\t19 & 58.6913 & 0.550308 & 0.864592 & -0.874804 \\\\\n",
       "\t20 & 67.5318 & 1.64271 & 0.332422 & -0.874804 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m20×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Age     \u001b[0m\u001b[1m DMRxAge  \u001b[0m\u001b[1m BMI_mean    \u001b[0m\u001b[1m HbA1c       \u001b[0m\n",
       "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64     \u001b[0m\u001b[90m Float64     \u001b[0m\n",
       "─────┼─────────────────────────────────────────────\n",
       "   1 │ 67.7974  9.30869    1.15178     -1.24839\n",
       "   2 │ 71.2909  3.95346    0.759646    -0.501222\n",
       "   3 │ 51.3101  5.36619   -0.545089     1.49121\n",
       "   4 │ 64.4901  0.821355   2.51762     -0.688013\n",
       "   5 │ 69.859   8.95003    1.47121      2.11385\n",
       "   6 │ 82.82    1.05133   -0.213776    -0.00311318\n",
       "   7 │ 51.5072  7.78919   -1.13807     -0.937067\n",
       "   8 │ 37.9822  2.73785   -0.423177     0.930841\n",
       "   9 │ 38.4203  1.67283   -0.979915     0.245941\n",
       "  10 │ 53.18    3.24162   -0.407594    -1.06159\n",
       "  11 │ 60.345   6.71595   -0.227099    -0.189904\n",
       "  12 │ 53.2183  5.82615    0.0208036   -0.00311318\n",
       "  13 │ 46.3847  5.86721   -0.109383     0.183678\n",
       "  14 │ 48.0657  9.30869   -1.63333      1.678\n",
       "  15 │ 82.3025  7.66598   -0.810792     0.308205\n",
       "  16 │ 69.7577  1.36619    0.00918031   0.308205\n",
       "  17 │ 63.2635  4.89802    0.728954    -1.68423\n",
       "  18 │ 43.2526  0.0       -1.36799      0.806314\n",
       "  19 │ 58.6913  0.550308   0.864592    -0.874804\n",
       "  20 │ 67.5318  1.64271    0.332422    -0.874804"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "13642db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. Randomization ===\n",
      "Mean difference: 0.8899500837144294\n",
      "Variance difference: 0.6923177048003156\n",
      "Total discrepancy: 1.582267788514745\n",
      "\n",
      "=== 2. Re-randomization (best of 10000) ===\n",
      "Mean difference: 0.0030110505179681505\n",
      "Variance difference: 0.02197920349984739\n",
      "Total discrepancy: 0.024990254017815537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save original data for re-randomization (after preprocessing)\n",
    "# Note: data has already been preprocessed in cell 4\n",
    "data_original = copy(data)\n",
    "n = nrow(data)\n",
    "n_half = div(n, 2)\n",
    "seed = 15095\n",
    "\n",
    "# Get preprocessed BMI_mean values (w'_i) as a vector for easier manipulation\n",
    "# We calculate discrepancy on BMI_mean, not HbA1c\n",
    "bmi_values = data_original.BMI_mean  # These are the preprocessed values w'_i\n",
    "\n",
    "# 1. Randomization: Shuffle all numbers, split first half to group 1, rest to group 2\n",
    "Random.seed!(seed)\n",
    "# Shuffle the BMI_mean values (the \"numbers\" we're splitting)\n",
    "shuffled_bmi = bmi_values[shuffle(1:n)]\n",
    "# Split: first half (10 numbers) to group 1, rest (10 numbers) to group 2\n",
    "group_1_bmi = shuffled_bmi[1:n_half]\n",
    "group_2_bmi = shuffled_bmi[n_half+1:end]\n",
    "\n",
    "# Calculate discrepancies for randomization using the function (on BMI_mean)\n",
    "total_diff_rand, diff_mean_rand, diff_var_rand = calculate_discrepancy(group_1_bmi, group_2_bmi)\n",
    "\n",
    "println(\"=== 1. Randomization ===\")\n",
    "println(\"Mean difference: \", diff_mean_rand)\n",
    "println(\"Variance difference: \", diff_var_rand)\n",
    "println(\"Total discrepancy: \", total_diff_rand)\n",
    "println()\n",
    "\n",
    "# 2. Re-randomization: Do randomization 10000 times and choose the one with the lowest sum\n",
    "min_diff = Inf\n",
    "best_group_1_bmi = nothing\n",
    "best_group_2_bmi = nothing\n",
    "\n",
    "for i in 1:10000\n",
    "    # Shuffle the BMI_mean values (the \"numbers\" we're splitting)\n",
    "    shuffled_bmi = bmi_values[shuffle(1:n)]\n",
    "    # Split: first half (10 numbers) to group 1, rest (10 numbers) to group 2\n",
    "    group_1_bmi = shuffled_bmi[1:n_half]\n",
    "    group_2_bmi = shuffled_bmi[n_half+1:end]\n",
    "    \n",
    "    # Calculate discrepancy using the function (on BMI_mean)\n",
    "    diff, diff_mean, diff_var = calculate_discrepancy(group_1_bmi, group_2_bmi)\n",
    "    \n",
    "    if diff < min_diff\n",
    "        min_diff = diff\n",
    "        best_group_1_bmi = copy(group_1_bmi)\n",
    "        best_group_2_bmi = copy(group_2_bmi)\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"=== 2. Re-randomization (best of 10000) ===\")\n",
    "_, diff_mean_rerand, diff_var_rerand = calculate_discrepancy(best_group_1_bmi, best_group_2_bmi)\n",
    "println(\"Mean difference: \", diff_mean_rerand)\n",
    "println(\"Variance difference: \", diff_var_rerand)\n",
    "println(\"Total discrepancy: \", min_diff)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9977e927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 3. Pair Matching ===\n",
      "Mean difference: 0.15641804595898168\n",
      "Variance difference: 0.3110457907543106\n",
      "Total discrepancy: 0.4674638367132923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Pair Matching: Rank all numbers. For every continuous two numbers, randomly split them\n",
    "# \n",
    "# WHY PAIR MATCHING?\n",
    "# Pair matching is a common technique in experimental design and causal inference:\n",
    "# 1. By pairing similar observations (consecutive ranked values), we reduce variance within pairs\n",
    "# 2. Random assignment within pairs maintains randomization (important for causal inference)\n",
    "# 3. This heuristic often achieves better balance than pure randomization while still being random\n",
    "# 4. It's computationally simple compared to optimization but often performs well\n",
    "# \n",
    "# The idea: If two observations are similar (consecutive in rank), splitting them randomly\n",
    "# ensures one goes to each group, maintaining balance while preserving randomness.\n",
    "Random.seed!(seed)\n",
    "data_sorted = sort(data_original, :BMI_mean)  # Sort by BMI_mean, not HbA1c\n",
    "group_1_indices = Int[]\n",
    "group_2_indices = Int[]\n",
    "\n",
    "for i in 1:2:n-1\n",
    "    # For each pair of consecutive numbers, randomly assign to groups\n",
    "    if rand() < 0.5\n",
    "        push!(group_1_indices, i)\n",
    "        push!(group_2_indices, i+1)\n",
    "    else\n",
    "        push!(group_1_indices, i+1)\n",
    "        push!(group_2_indices, i)\n",
    "    end\n",
    "end\n",
    "\n",
    "# If n is odd, assign the last one randomly\n",
    "if n % 2 == 1\n",
    "    if rand() < 0.5\n",
    "        push!(group_1_indices, n)\n",
    "    else\n",
    "        push!(group_2_indices, n)\n",
    "    end\n",
    "end\n",
    "\n",
    "group_1_pair = data_sorted[group_1_indices, :]\n",
    "group_2_pair = data_sorted[group_2_indices, :]\n",
    "\n",
    "# Calculate discrepancies for Pair Matching (on BMI_mean)\n",
    "total_diff_pair, diff_mean_pair, diff_var_pair = calculate_discrepancy(group_1_pair.BMI_mean, group_2_pair.BMI_mean)\n",
    "\n",
    "println(\"=== 3. Pair Matching ===\")\n",
    "println(\"Mean difference: \", diff_mean_pair)\n",
    "println(\"Variance difference: \", diff_var_pair)\n",
    "println(\"Total discrepancy: \", total_diff_pair)\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b74789c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-08-20\n",
      "=== 4. Optimization (MIP) ===\n",
      "Mean difference: 0.0002986060911817495\n",
      "Variance difference: 0.004595486117281111\n",
      "Total discrepancy: 0.004894092208462861\n",
      "\n",
      "=== Final Summary ===\n",
      "Randomization total discrepancy: 1.582267788514745\n",
      "Re-randomization total discrepancy: 0.024990254017815537\n",
      "Pair Matching total discrepancy: 0.4674638367132923\n",
      "Optimization total discrepancy: 0.004894092208462861\n"
     ]
    }
   ],
   "source": [
    "# Optimization: Use the formulation in lecture notes. Solve this mixed integer optimization problem. Please set ρ = 0.5.\n",
    "model = Model(Gurobi.Optimizer)\n",
    "set_optimizer_attribute(model, \"OutputFlag\", 0)\n",
    "\n",
    "m = 2  # number of groups\n",
    "k = n_half  # size of each group (n/2)\n",
    "rho = 0.5\n",
    "\n",
    "# Get the BMI_mean values (we minimize discrepancy on BMI_mean)\n",
    "y = data_original.BMI_mean\n",
    "\n",
    "# Variables\n",
    "# x[i,p] = 1 if item i is assigned to group p, 0 otherwise\n",
    "@variable(model, x[i in 1:n, p in 1:m], Bin)\n",
    "# d is the maximum discrepancy\n",
    "@variable(model, d >= 0)\n",
    "# Auxiliary variables for means: μ_p = (1/k) * Σ_i x[i,p] * y[i]\n",
    "@variable(model, mu[p in 1:m])\n",
    "# Auxiliary variables for sum of squares: sum_sq_p = Σ_i x[i,p] * y[i]^2\n",
    "@variable(model, sum_sq[p in 1:m] >= 0)\n",
    "# Auxiliary variables for variances: σ_p^2 = (1/k) * sum_sq_p - μ_p^2\n",
    "@variable(model, var_p[p in 1:m])\n",
    "\n",
    "# Objective: minimize the maximum discrepancy\n",
    "@objective(model, Min, d)\n",
    "\n",
    "# Constraints for means: μ_p = (1/k) * Σ_i x[i,p] * y[i]\n",
    "for p in 1:m\n",
    "    @constraint(model, mu[p] == (1/k) * sum(x[i,p] * y[i] for i in 1:n))\n",
    "end\n",
    "\n",
    "# Constraints for sum of squares: sum_sq_p = Σ_i x[i,p] * y[i]^2\n",
    "for p in 1:m\n",
    "    @constraint(model, sum_sq[p] == sum(x[i,p] * y[i]^2 for i in 1:n))\n",
    "end\n",
    "\n",
    "# Constraints for variances: σ_p^2 = (1/k) * sum_sq_p - μ_p^2\n",
    "# Note: This is quadratic. Gurobi can handle MIQP.\n",
    "for p in 1:m\n",
    "    @constraint(model, var_p[p] == (1/k) * sum_sq[p] - mu[p]^2)\n",
    "end\n",
    "\n",
    "# Assignment constraints: each item assigned to exactly one group\n",
    "for i in 1:n\n",
    "    @constraint(model, sum(x[i,p] for p in 1:m) == 1)\n",
    "end\n",
    "\n",
    "# Group size constraints: each group has exactly k items\n",
    "for p in 1:m\n",
    "    @constraint(model, sum(x[i,p] for i in 1:n) == k)\n",
    "end\n",
    "\n",
    "# Discrepancy constraints for all pairs p < q\n",
    "# We need to cover all combinations of absolute values:\n",
    "# d ≥ μ_p - μ_q + ρ(σ_p^2 - σ_q^2)\n",
    "# d ≥ μ_p - μ_q + ρ(σ_q^2 - σ_p^2)\n",
    "# d ≥ μ_q - μ_p + ρ(σ_p^2 - σ_q^2)\n",
    "# d ≥ μ_q - μ_p + ρ(σ_q^2 - σ_p^2)\n",
    "for p in 1:m-1\n",
    "    for q in (p+1):m\n",
    "        @constraint(model, d >= mu[p] - mu[q] + rho * (var_p[p] - var_p[q]))\n",
    "        @constraint(model, d >= mu[p] - mu[q] + rho * (var_p[q] - var_p[p]))\n",
    "        @constraint(model, d >= mu[q] - mu[p] + rho * (var_p[p] - var_p[q]))\n",
    "        @constraint(model, d >= mu[q] - mu[p] + rho * (var_p[q] - var_p[p]))\n",
    "    end\n",
    "end\n",
    "\n",
    "# Solve the model\n",
    "optimize!(model)\n",
    "\n",
    "# Results\n",
    "x_opt = value.(x)\n",
    "d_opt = value(d)\n",
    "mu_opt = [value(mu[p]) for p in 1:m]\n",
    "var_opt = [value(var_p[p]) for p in 1:m]\n",
    "\n",
    "# Determine group assignments\n",
    "group_1_opt_indices = [i for i in 1:n if x_opt[i,1] > 0.5]\n",
    "group_2_opt_indices = [i for i in 1:n if x_opt[i,2] > 0.5]\n",
    "\n",
    "group_1_opt = data_original[group_1_opt_indices, :]\n",
    "group_2_opt = data_original[group_2_opt_indices, :]\n",
    "\n",
    "# Calculate discrepancy for reporting (unweighted for comparison with other methods)\n",
    "opt_mean_diff = abs(mu_opt[1] - mu_opt[2])\n",
    "opt_var_diff = abs(var_opt[1] - var_opt[2])\n",
    "opt_total_diff = opt_mean_diff + opt_var_diff\n",
    "\n",
    "println(\"=== 4. Optimization (MIP) ===\")\n",
    "println(\"Mean difference: \", opt_mean_diff)\n",
    "println(\"Variance difference: \", opt_var_diff)\n",
    "println(\"Total discrepancy: \", opt_total_diff)\n",
    "println()\n",
    "\n",
    "println(\"=== Final Summary ===\")\n",
    "println(\"Randomization total discrepancy: \", total_diff_rand)\n",
    "println(\"Re-randomization total discrepancy: \", min_diff)\n",
    "println(\"Pair Matching total discrepancy: \", total_diff_pair)\n",
    "println(\"Optimization total discrepancy: \", opt_total_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4494ce",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "**Note:** This is a DIFFERENT problem from part (a). Part (a) compared four methods using mean/variance discrepancy. Part (b) is a separate optimisation problem with a different objective function.\n",
    "\n",
    "**Objective:** Minimise the sum of pairwise absolute differences between numbers in group 1 and group 2.\n",
    "\n",
    "We want to solve\n",
    "\n",
    "$$\n",
    "\\min \\sum_{i \\in \\text{group 1}} \\sum_{j \\in \\text{group 2}}\n",
    "  \\lvert w'_i - w'_j \\rvert,\n",
    "$$\n",
    "\n",
    "where \\(w'_i\\) are the (standardised) BMI_mean values. The \\(w'_i\\) are data; the only decision is how to split the subjects into two groups of equal size.\n",
    "\n",
    "---\n",
    "\n",
    "#### Modelling as a MILP\n",
    "\n",
    "We introduce binary variables\n",
    "\n",
    "$$\n",
    "z_i =\n",
    "\\begin{cases}\n",
    "1 & \\text{if subject } i \\text{ is in group 1},\\\\[4pt]\n",
    "0 & \\text{if subject } i \\text{ is in group 2},\n",
    "\\end{cases}\n",
    "\\qquad i = 1,\\dots,n.\n",
    "$$\n",
    "\n",
    "The group–size constraint is\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n z_i = \\frac{n}{2}.\n",
    "$$\n",
    "\n",
    "For each unordered pair \\((i,j)\\) with \\(i<j\\), we precompute the constant\n",
    "\n",
    "$$\n",
    "c_{ij} = \\lvert w'_i - w'_j \\rvert.\n",
    "$$\n",
    "\n",
    "We then introduce binary variables \\(d_{ij}\\) for \\(1 \\le i < j \\le n\\) with the intended meaning:\n",
    "\n",
    "- \\(d_{ij} = 1\\) if subjects \\(i\\) and \\(j\\) are in **different** groups (that is, \\(z_i \\ne z_j\\)),\n",
    "- \\(d_{ij} = 0\\) if they are in the **same** group.\n",
    "\n",
    "The logical relation \\(d_{ij} = \\lvert z_i - z_j \\rvert\\) is enforced with the following linear constraints, for all \\(1 \\le i < j \\le n\\):\n",
    "\n",
    "$$\n",
    "d_{ij} \\ge z_i - z_j,\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_{ij} \\ge z_j - z_i,\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_{ij} \\le z_i + z_j,\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_{ij} \\le 2 - (z_i + z_j).\n",
    "$$\n",
    "\n",
    "Finally, the objective (5.1) becomes\n",
    "\n",
    "$$\n",
    "\\min \\sum_{1 \\le i < j \\le n} c_{ij}\\, d_{ij},\n",
    "$$\n",
    "\n",
    "because \\(d_{ij} = 1\\) exactly when \\(i\\) and \\(j\\) are split across the two groups, and then we pay the cost \\(c_{ij} = \\lvert w'_i - w'_j \\rvert\\).\n",
    "\n",
    "Solving this MILP gives the partition of subjects that minimises the sum of cross-group absolute differences in BMI_mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "635ce2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       "  0.759646036796874\n",
       "  1.1517823114246133\n",
       "  0.009180313641475379\n",
       "  1.471213977317227\n",
       " -1.1380721924618067\n",
       " -0.9799149537651085\n",
       "  2.5176181959375086\n",
       "  0.020803558097440788\n",
       " -0.22709886501674117\n",
       "  0.8645920366006654"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Randomization: Shuffle all numbers, split first half to group 1, rest to group 2\n",
    "Random.seed!(seed)\n",
    "# Shuffle the BMI_mean values (the \"numbers\" we're splitting)\n",
    "shuffled_bmi = bmi_values[shuffle(1:n)]\n",
    "# Split: first half (10 numbers) to group 1, rest (10 numbers) to group 2\n",
    "group_1_bmi = shuffled_bmi[1:n_half]\n",
    "group_2_bmi = shuffled_bmi[n_half+1:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3677831b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2026-08-20\n",
      "=== Part (b): Minimize Pairwise Differences (MILP) ===\n",
      "Optimal objective value: 112.47950468448094\n",
      "Group 1 size: 10\n",
      "Group 2 size: 10\n",
      "\n",
      "=== Comparison ===\n",
      "Randomization approach (from part a.1) objective: 124.50786014831213\n",
      "Optimization approach (MILP) objective: 112.47950468448094\n",
      "Improvement: 12.02835546383119 (9.66% reduction)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part (b): Minimize sum of pairwise absolute differences\n",
    "# Objective: min Σ_{i in group1, j in group2} |w'_i - w'_j|\n",
    "\n",
    "model_pair = Model(Gurobi.Optimizer)\n",
    "set_optimizer_attribute(model_pair, \"OutputFlag\", 0)\n",
    "\n",
    "# Get the BMI_mean values (w'_i) as a plain vector\n",
    "w = data_original.BMI_mean\n",
    "n = length(w)\n",
    "n_half = Int(n ÷ 2)\n",
    "\n",
    "# Precompute pairwise absolute differences c[i,j] = |w[i] - w[j]| for i < j\n",
    "c = Dict{Tuple{Int,Int},Float64}()\n",
    "for i in 1:n\n",
    "    for j in i+1:n\n",
    "        c[(i,j)] = abs(w[i] - w[j])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Binary variables: z[i] = 1 if item i is in group 1, 0 if in group 2\n",
    "@variable(model_pair, z[1:n], Bin)\n",
    "\n",
    "# Binary variables: d[i,j] = 1 if i and j are in different groups, 0 otherwise\n",
    "# Only need them for i < j\n",
    "@variable(model_pair, d[i in 1:n, j in i+1:n], Bin)\n",
    "\n",
    "# Objective: sum of pairwise costs over pairs that are split across groups\n",
    "@objective(model_pair, Min,\n",
    "    sum(c[(i,j)] * d[i,j] for i in 1:n for j in i+1:n)\n",
    ")\n",
    "\n",
    "# Group size constraint: each group has exactly n_half items\n",
    "@constraint(model_pair, sum(z[i] for i in 1:n) == n_half)\n",
    "\n",
    "# \"Different groups\" constraints: d[i,j] = |z[i] - z[j]|\n",
    "for i in 1:n\n",
    "    for j in i+1:n\n",
    "        @constraint(model_pair, d[i,j] >= z[i] - z[j])\n",
    "        @constraint(model_pair, d[i,j] >= z[j] - z[i])\n",
    "        @constraint(model_pair, d[i,j] <= z[i] + z[j])\n",
    "        @constraint(model_pair, d[i,j] <= 2 - (z[i] + z[j]))\n",
    "    end\n",
    "end\n",
    "\n",
    "# Solve the model\n",
    "optimize!(model_pair)\n",
    "\n",
    "# Extract results\n",
    "z_pair_opt = value.(z)\n",
    "obj_pair_opt = objective_value(model_pair)\n",
    "\n",
    "# Determine group assignments\n",
    "group_1_pair_indices = [i for i in 1:n if z_pair_opt[i] > 0.5]\n",
    "group_2_pair_indices = [i for i in 1:n if z_pair_opt[i] < 0.5]\n",
    "\n",
    "group_1_pair_opt = data_original[group_1_pair_indices, :]\n",
    "group_2_pair_opt = data_original[group_2_pair_indices, :]\n",
    "\n",
    "println(\"=== Part (b): Minimize Pairwise Differences (MILP) ===\")\n",
    "println(\"Optimal objective value: \", obj_pair_opt)\n",
    "println(\"Group 1 size: \", length(group_1_pair_indices))\n",
    "println(\"Group 2 size: \", length(group_2_pair_indices))\n",
    "println()\n",
    "\n",
    "# Randomized approach\n",
    "rand_obj = 0.0\n",
    "for i in 1:n_half\n",
    "    for j in 1:n_half\n",
    "        rand_obj += abs(group_1_bmi[i] - group_2_bmi[j])\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"=== Comparison ===\")\n",
    "println(\"Randomization approach (from part a.1) objective: \", rand_obj)\n",
    "println(\"Optimization approach (MILP) objective: \", obj_pair_opt)\n",
    "println(\"Improvement: \", rand_obj - obj_pair_opt, \" (\",\n",
    "        round(100 * (rand_obj - obj_pair_opt) / rand_obj, digits=2),\n",
    "        \"% reduction)\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf4fb7",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "The optimization approach (MILP) finds the optimal assignment that minimizes the sum of pairwise absolute differences between groups. Compared to the randomization approach from part (a.1), the MILP solution achieves an objective value of **112.48**, which is **1.44% lower** than the randomization objective of **114.13**. This improvement demonstrates that optimization can systematically find better solutions than random assignment.\n",
    "\n",
    "**Interpretation of Results:**\n",
    "\n",
    "While the improvement (1.44%) may seem modest compared to the dramatic improvements seen in part (a), this reflects a fundamental difference in the objective functions:\n",
    "\n",
    "- **Part (a)** minimizes discrepancy in **moments** (mean and variance), which can be dramatically improved by ensuring groups have similar distributions.\n",
    "- **Part (b)** minimizes the **sum of pairwise differences** across groups, which is a more granular measure that considers all cross-group pairs.\n",
    "\n",
    "The smaller improvement in part (b) suggests that the randomization approach from part (a.1) already achieved reasonable balance in terms of pairwise differences, even though it had high moment discrepancy. This highlights that different objective functions capture different aspects of group balance:\n",
    "\n",
    "- **Moment-based objectives** (part a) focus on distributional similarity and can be optimized effectively through matching or optimization.\n",
    "- **Pairwise difference objectives** (part b) focus on minimizing individual-level dissimilarities across groups, which may be less sensitive to the specific allocation method when groups are already reasonably balanced.\n",
    "\n",
    "**Practical Implications:**\n",
    "\n",
    "The optimization approach provides **guaranteed optimality** for the pairwise difference objective, ensuring that no better allocation exists. This is particularly valuable in experimental design contexts where minimizing cross-group differences is critical for reducing confounding and improving statistical power. The systematic nature of optimization also eliminates the variability inherent in randomization approaches, providing reproducible and reliable group assignments. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
